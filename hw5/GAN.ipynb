{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54564,
     "status": "ok",
     "timestamp": 1589529239196,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "Uk3M4tcp95hr",
    "outputId": "49bc63aa-c22e-4d2e-e2ce-d0c8c305a31b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped\n",
    "# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n",
    "# folders.\n",
    "FOLDERNAME = 'IntroDL/hw5/'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd drive/My\\ Drive\n",
    "%cp -r $FOLDERNAME ../../\n",
    "%cd ../..\n",
    "%cd 'hw5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vVwzvMr917s",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### What is a GAN?\n",
    "\n",
    "In 2014, [Goodfellow et al.](https://arxiv.org/abs/1406.2661) presented a method for training generative models called Generative Adversarial Networks (GANs for short). In a GAN, we build two different neural networks. Our first network is a traditional classification network, called the **discriminator**. We will train the discriminator to take images, and classify them as being real (belonging to the training set) or fake (not present in the training set). Our other network, called the **generator**, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.\n",
    "\n",
    "We can think of this back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "where $x \\sim p_\\text{data}$ are samples from the input data, $z \\sim p(z)$ are the random noise samples, $G(z)$ are the generated images using the neural network generator $G$, and $D$ is the output of the discriminator, specifying the probability of an input being real. In [Goodfellow et al.](https://arxiv.org/abs/1406.2661), they analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from $G$.\n",
    "\n",
    "To optimize this minimax game, we will aternate between taking gradient *descent* steps on the objective for $G$, and gradient *ascent* steps on the objective for $D$:\n",
    "1. update the **generator** ($G$) to minimize the probability of the __discriminator making the correct choice__. \n",
    "2. update the **discriminator** ($D$) to maximize the probability of the __discriminator making the correct choice__.\n",
    "\n",
    "While these updates are useful for analysis, they do not perform well in practice. Instead, we will use a different objective when we update the generator: maximize the probability of the **discriminator making the incorrect choice**. This small change helps to allevaiate problems with the generator gradient vanishing when the discriminator is confident. This is the standard update used in most GAN papers, and was used in the original paper from [Goodfellow et al.](https://arxiv.org/abs/1406.2661). \n",
    "\n",
    "In this assignment, we will alternate the following updates:\n",
    "1. Update the generator ($G$) to maximize the probability of the discriminator making the incorrect choice on generated data:\n",
    "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "2. Update the discriminator ($D$), to maximize the probability of the discriminator making the correct choice on real and generated data:\n",
    "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "### What else is there?\n",
    "Since 2014, GANs have exploded into a huge research area, with massive [workshops](https://sites.google.com/site/nips2016adversarial/), and [hundreds of new papers](https://github.com/hindupuravinash/the-gan-zoo). Compared to other approaches for generative models, they often produce the highest quality samples but are some of the most difficult and finicky models to train (see [this github repo](https://github.com/soumith/ganhacks) that contains a set of 17 hacks that are useful for getting models working). Improving the stabiilty and robustness of GAN training is an open research question, with new papers coming out every day! For a more recent tutorial on GANs, see [here](https://arxiv.org/abs/1701.00160). There is also some even more recent exciting work that changes the objective function to Wasserstein distance and yields much more stable results across model architectures: [WGAN](https://arxiv.org/abs/1701.07875), [WGAN-GP](https://arxiv.org/abs/1704.00028).\n",
    "\n",
    "\n",
    "GANs are not the only way to train a generative model! For other approaches to generative modeling check out the [deep generative model chapter](http://www.deeplearningbook.org/contents/generative_models.html) of the Deep Learning [book](http://www.deeplearningbook.org). \n",
    "\n",
    "Here's an example of what your outputs from the 3 different models you're going to train should look like... note that GANs are sometimes finicky, so your outputs might not look exactly like this... this is just meant to be a *rough* guideline of the kind of quality you can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1589529245057,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "oWW4PTi1-S-h",
    "outputId": "a5657e08-a7ee-4f53-b2db-8c9134ad28ec"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"gan_outputs_tf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXiTdLTN917t"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDKWk5w6917u",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# A bunch of utility functions\n",
    "\n",
    "def show_images(images):\n",
    "    images = images.view(images.shape[0], -1).detach().cpu().numpy()\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg, sqrtimg]))\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "answers = {}\n",
    "\n",
    "for k, v in np.load('gan-checks-tf.npz').items():\n",
    "    answers[k] = torch.tensor(v)\n",
    "\n",
    "NOISE_DIM = 10\n",
    "NUM_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2G9Ri11917x",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Dataset\n",
    " GANs are notoriously finicky with hyperparameters, and also require many training epochs. In order to make this assignment approachable without a GPU, we will be working on the MNIST dataset, which is 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). This was one of the first datasets used to train convolutional neural networks and it is fairly easy -- a standard CNN model can easily exceed 99% accuracy. \n",
    " \n",
    "\n",
    "**Heads-up**: Our MNIST wrapper returns images as vectors. That is, they're size (batch, 784). If you want to treat them as images, we have to resize them to (batch,28,28) or (batch,28,28,1). They are also type np.float32 and bounded [0,1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjGQbGGG917y",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # [0,1]\n",
    "])\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "batch_size = 16\n",
    "mnist_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3470,
     "status": "ok",
     "timestamp": 1589529252584,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "y5aROSep9170",
    "outputId": "8b5a09f5-37c7-47f5-bc97-28c6509beb93"
   },
   "outputs": [],
   "source": [
    "# Show a batch\n",
    "data_iter = iter(mnist_loader)\n",
    "images, labels = next(data_iter)\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJyp2zmZ9178"
   },
   "source": [
    "## Random Noise\n",
    "Generate a Torch `Tensor` containing uniform noise from -1 to 1 with shape `[batch_size, dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_awag-D9179"
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"Generate random uniform noise from -1 to 1.\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size of noise to generate\n",
    "    - dim: integer giving the dimension of the noise to generate\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor containing uniform noise in [-1, 1] with shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # TODO: sample and return noise\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return torch.rand(batch_size, dim) * 2 - 1\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZLYONA6918A"
   },
   "source": [
    "Make sure noise is the correct shape and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1589529265803,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "7ORhW2JM918A",
    "outputId": "038e4b39-3028-42ab-8188-7e3ea2a53029"
   },
   "outputs": [],
   "source": [
    "def test_sample_noise():\n",
    "    batch_size = 3\n",
    "    dim = 4\n",
    "    z = sample_noise(batch_size, dim)\n",
    "    # Check z has the correct shape\n",
    "    assert list(z.shape) == [batch_size, dim]\n",
    "    # Make sure z is a Tensor and not a numpy array\n",
    "    assert isinstance(z, torch.Tensor)\n",
    "    # Check that we get different noise for different evaluations\n",
    "    z1 = sample_noise(batch_size, dim)\n",
    "    z2 = sample_noise(batch_size, dim)\n",
    "    assert not np.array_equal(z1.numpy(), z2.numpy())\n",
    "    # Check that we get the correct range\n",
    "    assert np.all(z1.numpy() >= -1.0) and np.all(z1.numpy() <= 1.0)\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_sample_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0bTxPMh918C"
   },
   "source": [
    "## Discriminator\n",
    "Our first step is to build a discriminator. **Hint:** You should use the layers in `torch.nn` to build the model.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected layer with input size 784 and output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with output size 1 \n",
    " \n",
    "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the scores that each of the `batch_size` inputs is a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OKCCniK918D"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=784):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward function\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk5T5ph2918G"
   },
   "source": [
    "Test to make sure the number of parameters in the discriminator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1589529265805,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "Nvhcoekn918G",
    "outputId": "f8949d47-9ca0-4100-ca18-9e1fe506c1ff"
   },
   "outputs": [],
   "source": [
    "def test_discriminator(true_count=267009):\n",
    "    model = Discriminator()\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Incorrect number of parameters in discriminator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Correct number of parameters in discriminator.')\n",
    "        \n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3diMv_l918J"
   },
   "source": [
    "## Generator\n",
    "Now to build a generator. You should use the layers in `torch.nn` to construct the model. All fully connected layers should include bias terms. Note that you can use the tf.nn module to access activation functions. Once again, use the default initializers for parameters.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected layer with input size z.shape[1] (the number of noise dimensions) and output size 1024\n",
    " * `ReLU`\n",
    " * Fully connected layer with output size 1024 \n",
    " * `ReLU`\n",
    " * Fully connected layer with output size 784\n",
    " * `Tanh` (To restrict every element of the output to be in the range [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZNClVw4918K"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=NOISE_DIM):\n",
    "        super(Generator, self).__init__()\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TODO: implement forward function\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = self.model(z)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmeJ7u5A918M"
   },
   "source": [
    "Test to make sure the number of parameters in the generator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1589529265806,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "ekry7Oyu918M",
    "outputId": "884f5f44-cf2f-4aa1-9873-81730f4ee3d7"
   },
   "outputs": [],
   "source": [
    "def test_generator(true_count=1858320):\n",
    "    model = Generator(4)\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Incorrect number of parameters in generator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Correct number of parameters in generator.')\n",
    "        \n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiqRS-Uh918O"
   },
   "source": [
    "# GAN Loss\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses.\n",
    "\n",
    "**HINTS**: Use `torch.ones_like` and `torch.zeros_like` to generate labels for your discriminator. Use `torch.nn.BCEWithLogitsLoss` to help compute your loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0oDt594918P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def discriminator_loss(logits_real, logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the discriminator loss described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: Tensor of shape (N, 1) giving scores for the real data.\n",
    "    - logits_fake: Tensor of shape (N, 1) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Tensor containing (scalar) the loss for the discriminator.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    real_labels = torch.ones_like(logits_real)\n",
    "    fake_labels = torch.zeros_like(logits_fake)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_real = criterion(logits_real, real_labels)\n",
    "    \n",
    "    loss_fake = criterion(logits_fake, fake_labels)\n",
    "    \n",
    "    loss = loss_real + loss_fake\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    fake_labels = torch.ones_like(logits_fake)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss = criterion(logits_fake, fake_labels)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulkqi5z6918R"
   },
   "source": [
    "Test your GAN loss. Make sure both the generator and discriminator loss are correct. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1790,
     "status": "ok",
     "timestamp": 1589529270057,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "CGLLiZxG918S",
    "outputId": "612eafa4-3ed4-4d6b-acb8-e13b1cfe2b46"
   },
   "outputs": [],
   "source": [
    "def test_discriminator_loss(logits_real, logits_fake, d_loss_true):\n",
    "    d_loss = discriminator_loss(logits_real,\n",
    "                                logits_fake)\n",
    "    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true.numpy(), d_loss.numpy()))\n",
    "\n",
    "test_discriminator_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                        answers['d_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1589529270507,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "E0Gw5urP918U",
    "outputId": "4d83b471-ff33-4758-d7d5-86ec17095162"
   },
   "outputs": [],
   "source": [
    "def test_generator_loss(logits_fake, g_loss_true):\n",
    "    g_loss = generator_loss(logits_fake)\n",
    "    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true.numpy(), g_loss.numpy()))\n",
    "\n",
    "test_generator_loss(answers['logits_fake'], answers['g_loss_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9rOePo8918W"
   },
   "source": [
    "# Optimizing our loss\n",
    "Make an `Adam` optimizer with a 1e-3 learning rate, beta1=0.5 to mininize G_loss and D_loss separately. The trick of decreasing beta was shown to be effective in helping GANs converge in the [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) paper. In fact, with our current hyperparameters, if you set beta1 to the Tensorflow default of 0.9, there's a good chance your discriminator loss will go to zero and the generator will fail to learn entirely. In fact, this is a common failure mode in GANs; if your D(x) learns too fast (e.g. loss goes near zero), your G(z) is never able to learn. Often D(x) is trained with SGD with Momentum or RMSProp instead of Adam, but here we'll use Adam for both D(x) and G(z). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cD_5p9zd918W"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def get_solvers(D, G, learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"\n",
    "    Create Adam optimizers for GAN training in PyTorch.\n",
    "\n",
    "    Inputs:\n",
    "    - D: Discriminator (nn.Module)\n",
    "    - G: Generator (nn.Module)\n",
    "    - learning_rate: learning rate for both solvers\n",
    "    - beta1: beta1 value for Adam (1st moment decay)\n",
    "\n",
    "    Returns:\n",
    "    - D_solver: optimizer for the discriminator\n",
    "    - G_solver: optimizer for the generator\n",
    "    \"\"\"\n",
    "    D_solver = None\n",
    "    G_solver = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    D_solver = optim.Adam(D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "    G_solver = optim.Adam(G.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwZQWTBL918Y",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Training a GAN!\n",
    "Well that wasn't so hard, was it? After the first epoch, you should see fuzzy outlines, clear shapes as you approach epoch 3, and decent shapes, about half of which will be sharp and clearly recognizable as we pass epoch 5. In our case, we'll simply train D(x) and G(z) with one batch each every iteration. However, papers often experiment with different schedules of training D(x) and G(z), sometimes doing one for more steps than the other, or even training each one until the loss gets \"good enough\" and then switching to training the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwDoRGks918Z",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss,\n",
    "              show_every=20, print_every=20, batch_size=128, num_epochs=10,\n",
    "              noise_size=NOISE_DIM):\n",
    "    \"\"\"\n",
    "    Train a GAN in PyTorch.\n",
    "    \n",
    "    Inputs:\n",
    "    - D: Discriminator model (nn.Module)\n",
    "    - G: Generator model (nn.Module)\n",
    "    - D_solver: optimizer for Discriminator\n",
    "    - G_solver: optimizer for Generator\n",
    "    - discriminator_loss: function to compute D loss\n",
    "    - generator_loss: function to compute G loss\n",
    "    \"\"\"\n",
    "    D = D.to(device)\n",
    "    G = G.to(device)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataloader = DataLoader(\n",
    "        datasets.MNIST(root='./data', train=True, download=True, transform=transform),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, _ in dataloader:\n",
    "            real_data = x.view(x.size(0), -1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            # 1. Update Discriminator\n",
    "            # ---------------------\n",
    "            D_solver.zero_grad()\n",
    "            logits_real = D(preprocess_img(real_data))\n",
    "\n",
    "            g_fake_seed = sample_noise(batch_size, noise_size).to(device)\n",
    "            fake_images = G(g_fake_seed)\n",
    "            logits_fake = D(fake_images)\n",
    "\n",
    "            d_total_error = discriminator_loss(logits_real, logits_fake)\n",
    "            d_total_error.backward()\n",
    "            D_solver.step()\n",
    "\n",
    "            # ---------------------\n",
    "            # 2. Update Generator\n",
    "            # ---------------------\n",
    "            G_solver.zero_grad()\n",
    "            g_fake_seed = sample_noise(batch_size, noise_size).to(device)\n",
    "            fake_images = G(g_fake_seed)\n",
    "            gen_logits_fake = D(fake_images)\n",
    "\n",
    "            g_error = generator_loss(gen_logits_fake)\n",
    "            g_error.backward()\n",
    "            G_solver.step()\n",
    "\n",
    "            # ---------------------\n",
    "            # 3. Logging & Visualization\n",
    "            # ---------------------\n",
    "            if iter_count % show_every == 0:\n",
    "                print(f'Epoch: {epoch}, Iter: {iter_count}, D: {d_total_error.item():.4f}, G: {g_error.item():.4f}')\n",
    "                imgs_numpy = fake_images[:16].detach().cpu()\n",
    "                show_images(imgs_numpy)\n",
    "                plt.show()\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "    # ----- Final Visualization -----\n",
    "    z = sample_noise(batch_size, noise_size).to(device)\n",
    "    G_sample = G(z).detach().cpu()\n",
    "    print('Final images')\n",
    "    show_images(G_sample[:16])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Hti6mvR918b"
   },
   "source": [
    "#### Train your GAN! This should take about 10 minutes on a CPU, or about 2 minutes on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Ao9yLcy8eOmhMmTOvUY46KahzAxWgSYn"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 305435,
     "status": "ok",
     "timestamp": 1589369430958,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "4xuwgNbZ918b",
    "outputId": "1629842a-27b5-4ad4-ff73-bfcc60caa28d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the discriminator\n",
    "D = Discriminator()\n",
    "\n",
    "# Make the generator\n",
    "G = Generator()\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver, G_solver = get_solvers(D, G)\n",
    "\n",
    "# Run it!\n",
    "run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6enI99c918d"
   },
   "source": [
    "# Least Squares GAN\n",
    "We'll now look at [Least Squares GAN](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all we have to do is change the loss function and retrain the model. We'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "and the discriminator loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$\n",
    "\n",
    "\n",
    "**HINTS**: Instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing. When plugging in for $D(x)$ and $D(G(z))$ use the direct output from the discriminator (`score_real` and `score_fake`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_CvzVst918e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ls_discriminator_loss(scores_real, scores_fake):\n",
    "    \"\"\"\n",
    "    Compute the Least-Squares GAN loss for the discriminator.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_real: Tensor of shape (N, 1) giving scores for the real data.\n",
    "    - scores_fake: Tensor of shape (N, 1) giving scores for the fake data.\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: A Tensor containing the loss.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    loss_real = 0.5 * torch.mean((scores_real - 1) ** 2)\n",
    "    loss_fake = 0.5 * torch.mean(scores_fake ** 2)\n",
    "    loss = loss_real + loss_fake\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss\n",
    "\n",
    "def ls_generator_loss(scores_fake):\n",
    "    \"\"\"\n",
    "    Computes the Least-Squares GAN loss for the generator.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_fake: Tensor of shape (N, 1) giving scores for the fake data.\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: A Tensor containing the loss.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    loss = 0.5 * torch.mean((scores_fake - 1) ** 2)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9f1y3XTS918g"
   },
   "source": [
    "Test your LSGAN loss. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2438,
     "status": "ok",
     "timestamp": 1589529304547,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "FaO65BWN918g",
    "outputId": "e0eb4bc0-fbf0-4d9c-d8aa-987915e2f9ab"
   },
   "outputs": [],
   "source": [
    "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n",
    "    \n",
    "    d_loss = ls_discriminator_loss(score_real, score_fake)\n",
    "    g_loss = ls_generator_loss(score_fake)\n",
    "    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true.numpy(), d_loss.numpy()))\n",
    "    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true.numpy(), g_loss.numpy()))\n",
    "\n",
    "test_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_aPhq5m918j"
   },
   "source": [
    "Create new training steps so we instead minimize the LSGAN loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1mGsOd0TqP4KjVMy12hSSMO4mU8hIZQIB"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 558172,
     "status": "ok",
     "timestamp": 1589369683707,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "VDw91r_s918j",
    "outputId": "6e38488f-cf40-4e42-8e03-771bad440615"
   },
   "outputs": [],
   "source": [
    "# Make the discriminator\n",
    "D = Discriminator()\n",
    "\n",
    "# Make the generator\n",
    "G = Generator()\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver, G_solver = get_solvers(D, G)\n",
    "\n",
    "# Run it!\n",
    "run_a_gan(D, G, D_solver, G_solver, ls_discriminator_loss, ls_generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbiWciAV918l"
   },
   "source": [
    "# Deep Convolutional GANs\n",
    "In the first part of the notebook, we implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, we will implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where we use convolutional networks as our discriminators and generators.\n",
    "\n",
    "#### Discriminator\n",
    "We will use a discriminator inspired by the TensorFlow MNIST classification [tutorial](https://www.tensorflow.org/get_started/mnist/pros), which is able to get above 99% accuracy on the MNIST dataset fairly quickly. *Be sure to check the dimensions of x and reshape when needed*, fully connected blocks expect [N,D] Tensors while conv2d blocks expect [N,H,W,C] Tensors. Please use `tf.keras.layers` to define the following architecture:\n",
    "\n",
    "Architecture:\n",
    "* Conv2D: 32 Filters, 5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Conv2D: 64 Filters, 5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Flatten\n",
    "* Fully Connected with output size 4 x 4 x 64\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Fully Connected with output size 1\n",
    "\n",
    "Once again, please use biases for all convolutional and fully connected layers, and use the default parameter initializers. Note that a padding of 0 can be accomplished with the 'VALID' padding option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1589529326712,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "O0LmuxRw918l",
    "outputId": "b68144cd-da40-4ea7-b7b1-eb2d813cac69"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 64, 4 * 4 * 64)\n",
    "        self.fc2 = nn.Linear(4 * 4 * 64, 1)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement forward function\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 4 * 4 * 64)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = Discriminator()\n",
    "test_discriminator(1102721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "045_Lk_B918o"
   },
   "source": [
    "#### Generator\n",
    "For the generator, we will copy the architecture exactly from the [InfoGAN paper](https://arxiv.org/pdf/1606.03657.pdf). See Appendix C.1 MNIST. Please use `tf.keras.layers` for your implementation. You might find the documentation for [tf.keras.layers.Conv2DTranspose](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2DTranspose) useful. The architecture is as follows.\n",
    "\n",
    "Architecture:\n",
    "* Fully connected with output size 1024 \n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Fully connected with output size 7 x 7 x 128 \n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Resize into Image Tensor of size 7, 7, 128\n",
    "* Conv2D^T (transpose): 64 filters of 4x4, stride 2\n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Conv2d^T (transpose): 1 filter of 4x4, stride 2\n",
    "* `TanH`\n",
    "\n",
    "Once again, use biases for the fully connected and transpose convolutional layers. Please use the default initializers for your parameters. For padding, choose the 'same' option for transpose convolutions. For Batch Normalization, assume we are always in 'training' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2029,
     "status": "ok",
     "timestamp": 1589529329742,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "r5faidms918o",
    "outputId": "e1e6d270-8540-49b1-96f6-1210052fb129"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=NOISE_DIM):\n",
    "        super(Generator, self).__init__()\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.fc1 = nn.Linear(noise_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 7 * 7 * 128)\n",
    "        self.bn2 = nn.BatchNorm1d(7 * 7 * 128)\n",
    "        \n",
    "        self.conv_t1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_t2 = nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TODO: implement forward function\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = self.fc1(z)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "        \n",
    "        x = self.conv_t1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.conv_t2(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "test_generator(6580801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw0GSlJt918q"
   },
   "source": [
    "We have to recreate our network since we've changed our functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8cR4FXW918r"
   },
   "source": [
    "### Train and evaluate a DCGAN\n",
    "This is the one part of A3 that significantly benefits from using a GPU. It takes 3 minutes on a GPU for the requested five epochs. Or about 50 minutes on a dual core laptop on CPU (feel free to use 3 epochs if you do it on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1_u2kwduHsL6Qy5TSZTteK0O3SpxwSdTV"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 140002,
     "status": "ok",
     "timestamp": 1589530179100,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "AIiyoFE_918r",
    "outputId": "28de23a7-3d7c-432b-a5af-a17f08a40def"
   },
   "outputs": [],
   "source": [
    "# Make the discriminator\n",
    "D = Discriminator()\n",
    "\n",
    "# Make the generator\n",
    "G = Generator()\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver, G_solver = get_solvers(D, G)\n",
    "\n",
    "# Run it!\n",
    "run_a_gan(D, G, D_solver, G_solver, ls_discriminator_loss, ls_generator_loss, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uD1B0XziKAfL"
   },
   "source": [
    "### Inception score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35212,
     "status": "ok",
     "timestamp": 1589529775109,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "4pSuprJOJ_wW",
    "outputId": "0bf334f5-2634-402b-a69c-6346aea0a879"
   },
   "outputs": [],
   "source": [
    "# ----- Hyperparameters -----\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "# ----- Data Preparation -----\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f'{len(train_dataset)} train samples')\n",
    "print(f'{len(test_dataset)} test samples')\n",
    "\n",
    "# ----- Model Definition -----\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def prob(self, x):\n",
    "        x = self.forward(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        return prob\n",
    "\n",
    "model = MLPClassifier().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----- Training -----\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# ----- Evaluation -----\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "print('Test accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtrPBVmkK5aI"
   },
   "source": [
    "### Verify the trained classifier on the generated samples\n",
    "Generate samples and visually inspect if the predicted labels on the samples match the actual digits in generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1731,
     "status": "ok",
     "timestamp": 1589530212958,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "SzXj02PQKJBE",
    "outputId": "5870d627-7c7d-445a-f9c6-f7246ce951af"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = sample_noise(NUM_SAMPLES, NOISE_DIM).to(device)\n",
    "    G_sample = G(z)\n",
    "    G_sample = deprocess_img(G_sample)\n",
    "show_images(G_sample[:20].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1881,
     "status": "ok",
     "timestamp": 1589530233028,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "zM_LGzTCeuio",
    "outputId": "392b78f5-0d25-4109-8ae3-c97d3409e5c9"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    G_sample = G_sample.reshape(NUM_SAMPLES, 784)\n",
    "    print(np.argmax(model(G_sample[:20].to(device)).cpu().numpy(), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qi-p27Illia3"
   },
   "source": [
    "### Implement the inception score\n",
    "Implement Equation 1 in the reference [3]. Replace expectation in the equation with empirical average of `num_samples` samples. Don't forget the exponentiation at the end. You should get Inception score of at least 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1718,
     "status": "ok",
     "timestamp": 1589530259781,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "0Il66tWnlWUk",
    "outputId": "8baaa316-7545-4f5e-94f2-59fbe3d1a2a7"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # TODO: implement here\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    probs = F.softmax(model(G_sample.to(device)), dim=1).cpu().numpy()\n",
    "    \n",
    "    p_y = np.mean(probs, axis=0)\n",
    "    \n",
    "    kl_divs = []\n",
    "    for i in range(probs.shape[0]):\n",
    "        p = probs[i]\n",
    "        kl_div = np.sum(p * np.log(p / p_y + 1e-10))\n",
    "        kl_divs.append(kl_div)\n",
    "    \n",
    "    inception_score = np.exp(np.mean(kl_divs))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f'Inception score: {inception_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLRpgAIIlvjt"
   },
   "source": [
    "### Plot the histogram of predicted labels\n",
    "Let's additionally inspect the class diversity of the generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384,
     "status": "ok",
     "timestamp": 1589530267159,
     "user": {
      "displayName": "Hyun Oh Song",
      "photoUrl": "",
      "userId": "05584924055130615371"
     },
     "user_tz": -540
    },
    "id": "iUPjiuSilWi6",
    "outputId": "a53f68e2-bcb6-45b6-8604-65cc7ad74bec"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.hist(np.argmax(model(G_sample).cpu(), axis=-1),\n",
    "             bins=np.arange(11)-0.5, rwidth=0.8, density=True)\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8avlWBs918v",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## INLINE QUESTION 1\n",
    "\n",
    "We will look at an example to see why alternating minimization of the same objective (like in a GAN) can be tricky business.\n",
    "\n",
    "Consider $f(x,y)=xy$. What does $\\min_x\\max_y f(x,y)$ evaluate to? (Hint: minmax tries to minimize the maximum value achievable.)\n",
    "\n",
    "Now try to evaluate this function numerically for 6 steps, starting at the point $(1,1)$, \n",
    "by using alternating gradient (first updating y, then updating x using that updated y) with step size $1$. **Here step size is the learning_rate, and steps will be learning_rate * gradient.**\n",
    "You'll find that writing out the update step in terms of $x_t,y_t,x_{t+1},y_{t+1}$ will be useful.\n",
    "\n",
    "Breifly explain what $\\min_x\\max_y f(x,y)$ evaluates to and record the six pairs of explicit values for $(x_t,y_t)$ in the table below.\n",
    "\n",
    "### Your answer:\n",
    " \n",
    "# #  For $\\min_x\\max_y f(x,y) = xy$, the $\\max_y xy$ reaches its maximum value when $y$ approaches infinity. However, considering $\\min_x$, when $x=0$, the result is always 0, so $\\min_x\\max_y xy = 0$.\n",
    "# #\n",
    "# #  Calculating with alternating gradient descent:\n",
    "#  $\\nabla_y f(x,y) = x$, $\\nabla_x f(x,y) = y$\n",
    "#  $y_{t+1} = y_t + \\text{step size} \\cdot \\nabla_y f(x_t, y_t) = y_t + x_t$\n",
    "#  $x_{t+1} = x_t - \\text{step size} \\cdot \\nabla_x f(x_t, y_{t+1}) = x_t - y_{t+1} = x_t - (y_t + x_t) = -y_t$\n",
    "#\n",
    "#  $y_0$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ | $y_6$ \n",
    "#  ----- | ----- | ----- | ----- | ----- | ----- | ----- \n",
    "#    1   |   2   |   1   |  -1   |  -2   |  -1   |   1   \n",
    "#  $x_0$ | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | $x_6$ \n",
    "#    1   |  -1   |  -1   |   1   |   1   |  -1   |  -1   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VOExFvG918w",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## INLINE QUESTION 2\n",
    "Using this method, will we ever reach the optimal value? Why or why not?\n",
    "\n",
    "# ### 답변: \n",
    "# # Using this method, we cannot reach the optimal value. As seen in the calculations above, the values of $(x_t, y_t)$ repeat periodically, forming patterns like $(1,1)$, $(-1,2)$, $(-1,1)$, $(1,-1)$, $(1,-2)$, $(-1,-1)$, $(-1,1)$. This is because the algorithm cycles through these values instead of converging to the optimal value $(0,y)$. This phenomenon illustrates the instability that can also occur in GAN training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XMtCFqQ918w",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## INLINE QUESTION 3\n",
    "If the generator loss decreases during training while the discriminator loss stays at a constant high value from the start, is this a good sign? Why or why not? A qualitative answer is sufficient.\n",
    "\n",
    "### Your answer: \n",
    "# ### 답변:\n",
    "# # This is not a good sign. When the generator loss decreases while the discriminator loss remains consistently high from the beginning, it indicates that the discriminator is not learning properly. In ideal GAN training, the generator and discriminator should compete with each other and maintain a balance. If the discriminator fails to learn effectively, it cannot provide useful feedback to the generator, and consequently, the generator may be under the illusion that it's improving while actually producing low-quality samples. This can lead to mode collapse or a situation where the generator only exploits the weaknesses of the discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHKHjwZZ918w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generative_Adversarial_Networks_TF_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
