{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8T-izvL9nin3"
   },
   "source": [
    "## White-box attack exercise\n",
    "\n",
    "In this exercise, you will implement the following white-box attacks.\n",
    "1. Fast Gradient Sign Method (FGSM)\n",
    "2. Projected Gradient Descent (PGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChfvqDeKnin8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped\n",
    "# 'attacks', 'datasets', and 'pretrained' folders\n",
    "FOLDERNAME = '2025DL/hw6'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd /content/drive/MyDrive/$FOLDERNAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEvkNBvLqyBc"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "%mkdir pretrained\n",
    "%cd pretrained\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1lA87UyuGpUiUCytKhwva_JnU_l-luPiX/view?usp=sharing\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QWlkdwZnioB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from cifar10_input import CIFAR10Data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0mQgSGmOnioC"
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8-cnJscnioC"
   },
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mJWH7fFknioD"
   },
   "source": [
    "## Loading Cifar-10 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPYGjJUpnioD"
   },
   "outputs": [],
   "source": [
    "from mean_std import mean_torch, std_torch\n",
    "\n",
    "mean_torch = mean_torch.to(device=device)\n",
    "std_torch = std_torch.to(device=device)\n",
    "\n",
    "# Transform the test set to pytorch Tensor without augmentation\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./datasets', train=False, download=True, \n",
    "                            transform=transform_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_13NbUZtnioE"
   },
   "source": [
    "Let's, visualize training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBu3Fio0nioF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(images, labels):\n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(2, (len(images) + 1) // 2, i + 1)\n",
    "        sample_image = images[i]\n",
    "        sample_label = labels[i]\n",
    "        plt.imshow(sample_image.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        plt.title(classes[sample_label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15TXCfgJnioF"
   },
   "outputs": [],
   "source": [
    "test_samples = [cifar10_test.data[i] for i in range(10)]\n",
    "test_labels = [cifar10_test.targets[i] for i in range(10)]\n",
    "print(\"Test Data:\")\n",
    "visualize(test_samples, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f8qq18yPnioG"
   },
   "source": [
    "In this exercise, we use a PreActResNet18 model ([arxiv](https://arxiv.org/abs/1603.05027)), which is one of ResNet-type models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjI-jSpEnioH"
   },
   "outputs": [],
   "source": [
    "from models import resnet50 as resnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tiSQrenZnioH"
   },
   "source": [
    "Next, we define model with mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1vzxvtwnioI"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "print('Creating a ResNet model')\n",
    "model = resnet(mean_torch, std_torch).to(device)\n",
    "\n",
    "# Load an naturally-trained model\n",
    "print('Loading pre-trained model')\n",
    "state_dict = torch.load('./pretrained/vanilla.pt', map_location='cpu')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Se9t4T5enioI"
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Before implementing attack methods, we have to evaluate the model for the following reasons.\n",
    "1. To check whether the model is successfuly restored. \n",
    "2. To get samples that are correctly classified. We don't have to attack misclassified samples.\n",
    "\n",
    "Note that the indices of the first 100 samples are stored in a variable named `correct_indices`. You will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK84kXxvnioJ"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, indices, attack_method=None):\n",
    "    \"\"\"\n",
    "    Given the data specified by the indices, evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model\n",
    "        dataset: Cifar-10 test dataset\n",
    "        indices: Indices that specifies the data\n",
    "        attack_method (optional): Instance of attack method, If it is not None, the attack method is applied before\n",
    "        evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        is_correct: list of 0 or 1. 1 if ith image was correctly predicted and 0 otherwise\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    is_correct = np.zeros([0], np.int32)\n",
    "    num_images = len(indices)\n",
    "    batch_size = 100\n",
    "    num_batches = int(math.ceil(num_images / batch_size))\n",
    "    \n",
    "    # Run batches\n",
    "    for batch in range(num_batches):\n",
    "        # Construct batch\n",
    "        bstart = batch * batch_size\n",
    "        bend = min(bstart + batch_size, num_images)\n",
    "        \n",
    "        image_batch = dataset.data[indices[bstart:bend]]\n",
    "        image_batch = torch.Tensor(np.transpose(image_batch, (0, 3, 1, 2))).to(device=device)\n",
    "\n",
    "        label_batch = np.array(dataset.targets)[indices[bstart:bend]]\n",
    "        label_batch = torch.Tensor(label_batch)\n",
    "        label_batch = label_batch.to(dtype=torch.int64)\n",
    "        \n",
    "        # Attack batch\n",
    "        if attack_method is not None:\n",
    "            image_batch = attack_method.perturb(image_batch, label_batch)\n",
    "            \n",
    "        # Evaluate batch\n",
    "        logit = model(image_batch)\n",
    "        _, predicted = torch.max(logit.data, 1)\n",
    "        \n",
    "        correct_prediction = (predicted.cpu().numpy() == label_batch.numpy())\n",
    "        is_correct = np.concatenate([is_correct, correct_prediction], axis=0)\n",
    "    \n",
    "    return is_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b52dpFw2nioK"
   },
   "outputs": [],
   "source": [
    "print('Evaluating naturally-trained model')\n",
    "is_correct = evaluate(model, cifar10_test, np.arange(0, 1000))\n",
    "\n",
    "print('Accuracy: {:.1f}%'.format(sum(is_correct) / len(is_correct) * 100))\n",
    "\n",
    "correct_indices = np.where(is_correct == 1)[0][:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4_agALrKnioL"
   },
   "source": [
    "## Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "Now, you will implement Fast Gradient Sign Method under $\\ell_{\\infty}$ constraint, the first method of generating adversarial examples proposed by [Goodfellow et al.](https://arxiv.org/abs/1412.6572). The algorithm is as follows.\n",
    "\n",
    "<center>$x_{adv} = x + \\epsilon \\cdot \\text{sgn}(\\nabla_{x} L(x, y, \\theta))$</center>\n",
    "\n",
    "where $x, y$ are an image and the corresponding label, $L$ is a loss function, and $\\epsilon$ is a maximum perturbation. Usually, Cross-Entropy loss is used for $L$. However, there might be many possible choices for $L$, such as Carlini-Wagner loss (https://arxiv.org/abs/1608.04644)\n",
    "\n",
    "Your code for this section will all be written inside `attacks/fgsm_attack.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdvGmj4WnioN"
   },
   "outputs": [],
   "source": [
    "# First implement Fast Gradient Sign Method.\n",
    "# Open attacks/fgsm_attack.py and follow instructions in the file.\n",
    "\n",
    "from attacks.fgsm_attack import FGSMAttack\n",
    "\n",
    "# Check if your implementation is correct.\n",
    "\n",
    "# Default attack setting\n",
    "epsilon = 2\n",
    "loss_func = 'xent'\n",
    "\n",
    "# Create an instance of FGSMAttack\n",
    "fgsm_attack = FGSMAttack(model, epsilon, loss_func, device)\n",
    "\n",
    "# Run FGSM attack on a sample\n",
    "dataset = cifar10_test\n",
    "index = 0\n",
    "sample_image = np.transpose(dataset.data[correct_indices[index]], (2, 0, 1))\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "sample_image = torch.Tensor(sample_image)\n",
    "\n",
    "sample_label = dataset.targets[correct_indices[index]]\n",
    "sample_label = np.expand_dims(sample_label, axis=0)\n",
    "sample_label = torch.Tensor(sample_label).to(dtype=torch.int64)\n",
    "\n",
    "sample_adv_image = fgsm_attack.perturb(sample_image, sample_label)\n",
    "_, sample_adv_label = torch.max(model(sample_adv_image).data, 1)\n",
    "\n",
    "sample_image = sample_image.cpu().detach().numpy()\n",
    "sample_adv_image = sample_adv_image.cpu().detach().numpy()\n",
    "\n",
    "assert np.amax(np.abs(sample_image - sample_adv_image)) <= epsilon\n",
    "assert np.amin(sample_adv_image) >= 0\n",
    "assert np.amax(sample_adv_image) <= 255\n",
    "\n",
    "# Plot the original image\n",
    "sample_image = [np.transpose(image, (1, 2, 0)) for image in sample_image]\n",
    "visualize(sample_image, sample_label)\n",
    "\n",
    "# Plot the adversarial image\n",
    "sample_adv_image = [np.transpose(image, (1, 2, 0)) for image in sample_adv_image]\n",
    "visualize(sample_adv_image, sample_adv_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Af8BpYHInioO"
   },
   "source": [
    "## Evaluating the performance of FGSM with varying $\\epsilon$\n",
    "\n",
    "Now, you will evaluate the performance of FGSM with varying a maximum perturbation $\\epsilon \\in [2, 4, 6, 8, 10]$. In this section, you will use Cross-Entropy loss as $L$. The procedure is as follows.\n",
    "\n",
    "1. Given $\\epsilon$, create an instance of FGSMAttack.\n",
    "2. Evaluate the performance of the attack instance over the samples specified by the variable `correct_indices`.\n",
    "3. Calculate attack success rate, which is defined by\n",
    "<center>$\\text{attack success rate}(\\%)=\\frac{\\# \\text{ samples that are successfully fooled}}{\\# \\text{ samples}}\\times 100$</center>\n",
    "4. Run 1, 2, and 3 for each $\\epsilon\\in [2, 4, 6, 8, 10]$ and draw a plot of attack success rate against $\\epsilon$.\n",
    "\n",
    "If correctly implemented, the success rate will be 75% or higher on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf8NgrlnnioQ"
   },
   "outputs": [],
   "source": [
    "criterion = 'xent'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SVId2vqtnioR"
   },
   "source": [
    "## Evaluating the performance of FGSM with Carlini-Wagner loss\n",
    "\n",
    "In this section, you will evaluate the performance of FGSM using Carlini-Wagner loss. Repeat the procedure in the previous section and compare the results.\n",
    "\n",
    "If correctly implemented, the success rate will be 80% or higher on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7cLZN42nioS"
   },
   "outputs": [],
   "source": [
    "criterion = 'cw'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-ro', label='FGSM (cw loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XUtiiZvnnioV"
   },
   "source": [
    "**Inline Question**\n",
    "\n",
    "Which is better, Cross-Entropy loss or Carlini-Wagner loss?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "khAwN6O4nioW"
   },
   "source": [
    "**Your Answer**\n",
    "\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sikSiFvinioS"
   },
   "source": [
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "Next, you will implement Projected Gradient Descent under $\\ell_{\\infty}$ constraint, which is considered as the strongest white-box attack method proposed by [Madry et al.](https://arxiv.org/abs/1706.06083). The algorithm is as follows.\n",
    "\n",
    "<center>$x^0 = x + \\delta, ~~ \\delta \\sim U(-\\epsilon, \\epsilon)$</center>\n",
    "<center>$x^{t+1} = \\prod_{B_{\\infty}(x, \\epsilon)} [x^{t} + \\alpha \\cdot \\text{sgn}(\\nabla_{x} L(x^{t}, y, \\theta))]$</center>\n",
    "\n",
    "where $x, y$ are an image and the corresponding label, $L$ is a loss function, $\\alpha$ is a step size, $\\epsilon$ is a maximum perturbation, and $B_{\\infty}(x, \\epsilon)$ is a $\\ell_\\infty$ ball of radius $\\epsilon$ centered at $x$.\n",
    "\n",
    "Your code for this section will all be written inside `attacks/pgd_attack.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypx-21xxnioT"
   },
   "outputs": [],
   "source": [
    "# First implement Projected Gradient Descent.\n",
    "# Open attacks/pgd_attack.py and follow instructions in the file.\n",
    "\n",
    "from attacks.pgd_attack import PGDAttack\n",
    "\n",
    "# Check if your implementation is correct.\n",
    "\n",
    "# Default attack setting\n",
    "epsilon = 2\n",
    "step_size = 2\n",
    "num_steps = 20\n",
    "criterion = 'xent'\n",
    "\n",
    "# Create an instance of FGSMAttack\n",
    "pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, criterion, device)\n",
    "\n",
    "# Run PGD attack on a sample\n",
    "dataset = cifar10_test\n",
    "index = 0\n",
    "sample_image = np.transpose(dataset.data[correct_indices[index]], (2, 0, 1))\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "sample_image = torch.Tensor(sample_image)\n",
    "\n",
    "sample_label = dataset.targets[correct_indices[index]]\n",
    "sample_label = np.expand_dims(sample_label, axis=0)\n",
    "sample_label = torch.Tensor(sample_label).to(dtype=torch.int64)\n",
    "\n",
    "sample_adv_image = pgd_attack.perturb(sample_image, sample_label)\n",
    "_, sample_adv_label = torch.max(model(sample_adv_image), 1)\n",
    "\n",
    "sample_image = sample_image.cpu().detach().numpy()\n",
    "sample_adv_image = sample_adv_image.cpu().detach().numpy()\n",
    "\n",
    "# Check if the adversarial image is valid\n",
    "assert np.amax(np.abs(sample_image - sample_adv_image)) <= epsilon\n",
    "assert np.amin(sample_adv_image) >= 0\n",
    "assert np.amax(sample_adv_image) <= 255\n",
    "\n",
    "# Plot the original image\n",
    "sample_image = [np.transpose(image, (1, 2, 0)) for image in sample_image]\n",
    "visualize(sample_image, sample_label)\n",
    "\n",
    "# Plot the adversarial image\n",
    "sample_adv_image = [np.transpose(image, (1, 2, 0)) for image in sample_adv_image]\n",
    "visualize(sample_adv_image, sample_adv_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-NlhMAbdnioT"
   },
   "source": [
    "## Evaluating the performance of PGD with varying $\\epsilon$\n",
    "\n",
    "Now, you will evaluate the performance of PGD with varying maximum perturbation $\\epsilon \\in [2, 4, 6, 8, 10]$. In this section, you will use Cross-Entropy loss as $L$. Step size and the number of iterations are set to 2 and 20 respectively. The procedure is as follows.\n",
    "\n",
    "1. First, create an instance of PGDAttack with given $\\epsilon$.\n",
    "2. Evaluate the performance of the attack instance over the samples specified by the variable `correct_indices`.\n",
    "3. Calculate attack success rate, which is defined by\n",
    "<center>$\\text{attack success rate}(\\%)=\\frac{\\# \\text{ samples that are successfully fooled}}{\\# \\text{ samples}}\\times 100$</center>\n",
    "4. Run 1, 2, and 4 for each $\\epsilon\\in [2, 4, 6, 8, 10]$ and draw a plot of attack success rate against $\\epsilon$.\n",
    "\n",
    "If correctly implemented, the success rate will be 100% on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lV7n5F_nioU"
   },
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "criterion = 'xent'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-bo', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WtBV-3TDnioV"
   },
   "source": [
    "## Evaluating the performance of PGD with Carlini-Wagner loss\n",
    "\n",
    "In this section, you will evaluate the performance of PGD using Carlini-Wagner loss. Repeat the procedure in the previous section and compare the results.\n",
    "\n",
    "If correctly implemented, the success rate will be 100% on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHU4DCXvnioV"
   },
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "criterion = 'cw'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-ro', label='PGD (cw loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RxgTjeCrnioW"
   },
   "source": [
    "## Attacks on adversarially-trained model\n",
    "\n",
    "As you can see, naturally-trained neural networks are vulnerable to adversarial attacks. There are several ways to improve adversarial robustness of neural networks. One example is adversarial training, which uses adversarial samples to train a neural network. It constitutes the current state-of-the-art in the adversarial robustness.\n",
    "\n",
    "PGD adversarial training, proposed by [Madry et al.](https://arxiv.org/abs/1706.06083), utilizes Projected Gradient Descent to train a network. It has been shown that PGD adversarial training on MNIST and Cifar-10 can defend white-box attack successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd pretrained\n",
    "!gdown --fuzzy https://drive.google.com/file/d/16ffe2zzHIetYRCPkrv5P3NMKrOnx-ePz/view?usp=sharing\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sY69KW5nioW"
   },
   "outputs": [],
   "source": [
    "# Create a naturally-trained model\n",
    "print('Creating a ResNet model')\n",
    "model = resnet(mean_torch, std_torch).to(device)\n",
    "\n",
    "# Load an advarsarially-trained model\n",
    "print('Loading an adversarially-trained model')\n",
    "state_dict = torch.load(\"./pretrained/adv.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BAbkoHsQnioX"
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Before implementing attack methods, we have to evaluate the model for the following reasons.\n",
    "1. To check whether the model is successfuly restored. \n",
    "2. To get samples that are correctly classified. We don't have to attack misclassified samples.\n",
    "\n",
    "Note that the indices of the first 100 samples are stored in a variable named `correct_indices`. You will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fLHRr21nioX"
   },
   "outputs": [],
   "source": [
    "# Evaluate the adversarially-trained model on the first 1000 samples in the test dataset\n",
    "indices = np.arange(0, 1000)\n",
    "\n",
    "print('Evaluating adversarially-trained model')\n",
    "correct_predictions = evaluate(model, cifar10_test, indices)\n",
    "accuracy = np.mean(correct_predictions) * 100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))\n",
    "\n",
    "# Select the first 100 samples that are correctly classified.\n",
    "correct_indices = np.where(correct_predictions==1)[0][:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ltDhfZCSnioX"
   },
   "source": [
    "**Inline Question**\n",
    "\n",
    "Is the accuracy of adversarially-trained model higher than that of naturally-trained model, or lower? Explain why they are different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fGM4oPc0nioX"
   },
   "source": [
    "**Your answer**\n",
    "\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3CbS3lVSnioY"
   },
   "source": [
    "**Useful material**\n",
    "\n",
    "For those who are curious about this phenomenon, see https://arxiv.org/abs/1805.12152."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zFEFYFWCnioY"
   },
   "source": [
    "## Evaluating the performance of FGSM on adversarially-trained model\n",
    "\n",
    "Now, we will evaluate the the performance of FGSM on adversarially-trained model. In this section, you will use Cross-Entropy loss as $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrgpE0-tnioY"
   },
   "outputs": [],
   "source": [
    "criterion = 'xent'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "fgsm_attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    fgsm_attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, fgsm_attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xohCpwOSnioZ"
   },
   "source": [
    "## Evaluating the performance of PGD on adversarially-trained model\n",
    "\n",
    "Now, we will evaluate the the performance of PGD on adversarially-trained model. In this section, you will use Cross-Entropy loss as $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsVTw48-nioa"
   },
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "criterion = 'xent'\n",
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "pgd_attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, criterion, device)\n",
    "    is_correct = evaluate(model, cifar10_test, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1 - is_correct) * 100\n",
    "    pgd_attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, pgd_attack_success_rates, '-ro', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kvzamZKUniob"
   },
   "source": [
    "## Comparing the performance of FGSM and PGD\n",
    "\n",
    "Finally, we compare the performace of FGSM and PGD on adversarially-trained model. Just overlay the plots drawn in the two previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jQ4Dyx_niob"
   },
   "outputs": [],
   "source": [
    "epsilons = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "plt.plot(epsilons, fgsm_attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.plot(epsilons, pgd_attack_success_rates, '-ro', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dt5Icbxjnioc"
   },
   "source": [
    "**Inline question**\n",
    "\n",
    "Describe the result above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4TNWJ_kvniod"
   },
   "source": [
    "**Your answer**\n",
    "\n",
    "위 결과를 보면 FGSM과 PGD 공격 모두 epsilon 값이 증가함에 따라 공격 성공률이 증가하는 것을 확인할 수 있습니다. \n",
    "\n",
    "epsilon이 0일 때는 두 공격 모두 성공률이 0%로, 실제로 공격이 이루어지지 않았습니다.\n",
    "\n",
    "epsilon이 낮은 값(2, 4)에서는 FGSM과 PGD의 성능이 비슷하게 나타납니다(epsilon=2에서 둘 다 9%, epsilon=4에서 FGSM 19%, PGD 20%).\n",
    "\n",
    "하지만 epsilon 값이 커질수록(6, 8, 10) PGD 공격이 FGSM보다 더 효과적임을 알 수 있습니다. 특히 epsilon=8에서 FGSM은 34%의 성공률을 보인 반면, PGD는 48%로 상당한 차이를 보입니다. epsilon=10에서도 FGSM(44%)보다 PGD(53%)가 더 높은 성공률을 보입니다.\n",
    "\n",
    "이는 PGD 공격이 여러 반복 단계를 통해 더 최적화된 적대적 예제를 생성하기 때문입니다. FGSM은 한 번의 그래디언트 계산으로 공격을 수행하는 반면, PGD는 여러 번의 반복적인 최적화 과정을 통해 더 효과적인 적대적 예제를 찾아냅니다. 따라서 제한된 섭동 범위(epsilon) 내에서도 PGD가 더 강력한 공격을 수행할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "white-box-attack.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9dd721613fb21c58a58d9d83cf55d6352974469ab93d835a501eeacc00c44696"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
