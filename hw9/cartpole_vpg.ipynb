{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uWru02gbwSiD"
   },
   "source": [
    "# Vanilla Policy Gradient\n",
    "\n",
    "## Formulation\n",
    "In this exercise, we will train a RL agent with Vanilla Policy Gradient (VPG), the simplest on-policy RL algorithm. The objective of VP is to maximize the expected return of the trajectories sampled from a policy $\\pi_\\theta$, which is expressed by\n",
    "$$ \\eta(\\pi_\\theta) = \\max_\\theta \\mathbb{E}_{\\tau \\sim p_{\\pi_\\theta}(\\tau)} \\left[ R(\\tau) \\right], $$\n",
    "where $R(\\tau)$ is the discounted return of a trajectory $\\tau$ of lenght $T$. Using the log-derivative trick, we can compute the gradient of the objective with respect to $\\theta$, which is given by\n",
    "$$ \\nabla_\\theta \\eta(\\pi_\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^m \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta (a_t^i \\mid s_t^i) \\right) \\left( \\sum_{t=0}^{T-1} \\gamma^t r(s_t^i, a_t^i) \\right), $$\n",
    "where $m$ is the number of trajectories sampled for training and $r(s, a)$ is an immediate reward given a state $s$ and an action $a$. Using the fact that the policy cannot affect rewards in the past, we can modify the gradient above as \n",
    "$$ \\nabla_\\theta \\eta(\\pi_\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^m \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta (a_t^i \\mid s_t^i) \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) \\right), $$\n",
    " where the sum of rewards here does not include the rewards achieved prior to the time step at which the policy is being queried.\n",
    "\n",
    "## Baseline\n",
    "In fact, the policy gradient suffers from high variance. To address this, we introduce a baseline function and subtract the baseline from the sum of rewards. Note that this does not affact the value of the objective by EGLP lemma. The most common choice of baseline is the on-policy value function $V_\\phi^\\pi$, which acts as a state-dependent baseline. The value function will be trained to approximate the discounted sum of future rewards starting from a particular state:\n",
    "$$ V_\\phi^\\pi(s_t) \\approx \\sum_{t'=t}^{T-1} \\gamma^{t'-t} \\mathbb{E}_{\\pi_\\theta} \\left[ r(s_{t'}, a_{t'}) \\mid s_t \\right]. $$\n",
    "Finally, the policy gradient now looks like as follows:\n",
    "$$ \\nabla_\\theta \\eta(\\pi_\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^m \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta (a_t^i \\mid s_t^i) \\underbrace{\\left( \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) - V_\\phi^\\pi(s_t^i) \\right)}_{A(s_{t'}^i, a_{t'}^i)} \\right), $$\n",
    "where $A$ is called advantage function. In practice, we use the standardized version of advantages.\n",
    "\n",
    "## Implementation\n",
    "To implement Vanila Policy Gradient, you need to fill in some blanks that are marked with `TODO` in the following files:\n",
    "- `MLPPolicyPG` (rlkit/policies/mlp_policy.py): a class for MLP policy for VPG, which takes an observation as an input and outputs an action.\n",
    "- `PGAgent` (rlkit/agents/pg_agent.py): a class for VPG agent, which updates the policy via VPG using given trajectories.\n",
    "\n",
    "### Implementing `MLPPolicyPG`\n",
    "\n",
    "MLPPolicyPG consists of policy and baseline network (optional), each of which is a feed-forward deep neural network. You should implement a method named `update`, whose functionality is to compute the losses for the policy and baseline and update the networks. \n",
    "\n",
    "### Implementing `PGAgent`\n",
    "\n",
    "PGAgent computes baselines, discounted returns, and advantages from given trajectories and passes them to `self.actor`, an instance of the `MLPPolicyPG` class. You should implement methods such as `calculate_baselines` and `calculate_advantages` to build the whole training procedure of a PG agent given trajectory data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DKJ9CfLewjGH"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z6tgNXZTe_8"
   },
   "outputs": [],
   "source": [
    "#@title 1. Mount your Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped 'rlkit' folder\n",
    "FOLDERNAME = 'hw9'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd /content/drive/MyDrive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pl1ONPAEvFR8"
   },
   "outputs": [],
   "source": [
    "#@title 2. Install packages\n",
    "\n",
    "#@markdown Please run the follown script to install external Linux and Python packages.\n",
    "\n",
    "#@markdown This may take a few minutes.\n",
    "\n",
    "!apt update \n",
    "!apt install xvfb ffmpeg\n",
    "\n",
    "!pip install tensorboard tensorboardX pyvirtualdisplay selenium\n",
    "!pip install gym==0.22.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GfQXQchFmi40"
   },
   "source": [
    "# Run Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yjfSjla2OTY"
   },
   "outputs": [],
   "source": [
    "#@title 1. Import packages\n",
    "\n",
    "import os\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "from rlkit.infrastructure.rl_trainer import OnPolicyRLTrainer\n",
    "from rlkit.agents.pg_agent import PGAgent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leDRIlAV1VUb"
   },
   "outputs": [],
   "source": [
    "#@title 2. Runtime arguments\n",
    "\n",
    "class Args:\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = \"CartPole-v1\" #@param\n",
    "  exp_name = \"vpg\" #@param\n",
    "\n",
    "  #@markdown main parameters of interest\n",
    "  n_iter = 200 #@param {type: \"integer\"}\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 500 #@param {type: \"integer\"}\n",
    "  discount = 0.99 #@param {type: \"number\"}\n",
    "  nn_baseline = True #@param {type: \"boolean\"}\n",
    "  standardize_advantages = True #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  n_trajs =  5#@param {type: \"integer\"}\n",
    "  eval_n_trajs = 5 #@param {type: \"integer\"}\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  learning_rate = 1e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown MLP parameters\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = True #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1337 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq = -1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZszdPDkne1Z"
   },
   "outputs": [],
   "source": [
    "#@title 3. Create directory for logging\n",
    "\n",
    "base_logdir = \"logs\"\n",
    "exp_name = args[\"exp_name\"] + '_' + args[\"env_name\"]\n",
    "logdir = os.path.join(base_logdir, exp_name)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "args[\"logdir\"] = logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvHuTGXTpWjZ"
   },
   "outputs": [],
   "source": [
    "#@title 4. Define policy gradient trainer\n",
    "\n",
    "class PG_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': params['standardize_advantages'],\n",
    "            'nn_baseline': params['nn_baseline'],\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = PGAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = OnPolicyRLTrainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhBBZV5OpicW"
   },
   "outputs": [],
   "source": [
    "#@title 5. Run training\n",
    "\n",
    "#@markdown If your implementation is correct, the average return will be above 400.\n",
    "\n",
    "#@markdown This may take a few minutes.\n",
    "\n",
    "trainer = PG_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3qdh9Sw45aN"
   },
   "outputs": [],
   "source": [
    "#@title 6. Run Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/$FOLDERNAME/logs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlnllzxSaTYNll+VhHLrjd",
   "collapsed_sections": [
    "uWru02gbwSiD"
   ],
   "name": "cartpole_vpg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
