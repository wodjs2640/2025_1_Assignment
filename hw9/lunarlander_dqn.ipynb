{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b8vjMJd0a_Nv"
   },
   "source": [
    "# Deep Q-Learning (DQN)\n",
    "\n",
    "This exercise requires you to implement and evaluate deep Q-learning (DQN) with convolutional neural networks for playing Atari games. The goal is to implement the DQN algorithm covered in the lecture and you will be provided with a starter code.\n",
    "\n",
    "For a quick reminder, DQN trains a parameterized Q-network $Q(\\cdot, \\cdot; \\phi_k)$ by minimizing the empirical Bellman error:\n",
    "\n",
    "$\\mathcal{E}(\\mathcal{D}, \\phi_{k,g}) = \\mathbb{E}_{(s,a,r,s')\\sim D} \\left[\\left(Q(s,a;\\phi_{k,g}) - \\left(\\underbrace{r + \\gamma \\max_{a'} Q(s',a';\\phi_k)}_{\\text{fixed parameter}~ \\phi_{k}}\\right)\\right)^2\\right],$\n",
    "\n",
    "where $\\mathcal{D}$, $k$, and $g$ each denotes the replay buffer, the iteration number, and the gradient step. The learned Q-network is then used to select the best action on a randomly given state:\n",
    "\n",
    "$a = \\text{argmax}_a Q(s, a; \\phi_K)$.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The default code will run the `LunarLander-v3` game with reasonable hyperparameter settings. You may want to look inside `rlkit/infrastructure/dqn_utils.py` to understand how the replay buffer works, but you do not need to modify it.\n",
    "\n",
    "In order to implement DQN, you will be writing new codes in the following files:\n",
    "\n",
    "> * `rlkit/agents/dqn_agent.py`\n",
    "> * `rlkit/critics/dqn_critic.py`\n",
    "> * `rlkit/policies/argmax_policy.py`\n",
    "\n",
    "In `rlkit/agents/dqn_agent.py`, you will implement some core parts of the DQN agent, including the epsilon-greedy exploration strategy, interacting with the environment, storing and retrieving from the replay buffer, etc.\n",
    "\n",
    "> * epsilon-greedy exploration: $\\pi_{k+1}(a | s) \\leftarrow \\epsilon \\mathcal{U}(a) + (1-\\epsilon) \\delta \\left(a = \\text{argmax}_a Q(s, a; \\phi_{k+1})\\right)$\n",
    "\n",
    "In `rlkit/critics/dqn_critic.py`, you will implement some core parts of the DQN Critic (Q-network), including Q-value estimation and Bellman error calculation.\n",
    "\n",
    "> * estimate error: $\\mathcal{E}(B, \\phi_{k,g}) = \\sum_{i\\in \\mathcal{I}} \\left[\\left(Q(s_i,a_i;\\phi_{k,g}) - \\left(r_i + \\gamma \\max_{a'} Q(s'_i,a';\\phi_k)\\right)\\right)^2\\right]$,\n",
    "\n",
    "where $B = \\{(s_i, a_i, s'_i, r_i)\\}_{i\\in\\mathcal{I}}$ is a random subset of the replay buffer $\\mathcal{D}$.\n",
    "\n",
    "In `rlkit/policies/argmax_policy.py`, you will implement the argmax policy which is used for selecting the best action in terms of maximizing the estimated Q-value.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XtMrlr2gcekL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzqWRp5ca8AP"
   },
   "outputs": [],
   "source": [
    "#@title 1. Mount your Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped 'rlkit' folder\n",
    "FOLDERNAME = 'hw9'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd /content/drive/MyDrive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNj6bdcrcjK9"
   },
   "outputs": [],
   "source": [
    "#@title 2. Install packages\n",
    "\n",
    "#@markdown Please run the follown script to install external Linux and Python packages.\n",
    "\n",
    "#@markdown This may take a few minutes.\n",
    "\n",
    "!apt update \n",
    "!apt install xvfb ffmpeg\n",
    "\n",
    "!pip install tensorboard tensorboardX pyvirtualdisplay selenium swig pyglet\n",
    "!pip install Box2D\n",
    "!pip install gym==0.22.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w-yGxOvCcsON"
   },
   "source": [
    "## Run DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9247,
     "status": "ok",
     "timestamp": 1627358619114,
     "user": {
      "displayName": "Seungyong Moon",
      "photoUrl": "",
      "userId": "08679509530392314245"
     },
     "user_tz": -540
    },
    "id": "pXNYRmMNcpp0"
   },
   "outputs": [],
   "source": [
    "#@title 1. Import packages\n",
    "\n",
    "import os\n",
    "from pyvirtualdisplay import Display\n",
    "import time\n",
    "\n",
    "from rlkit.infrastructure.rl_trainer import OffPolicyRLTrainer\n",
    "from rlkit.agents.dqn_agent import DQNAgent\n",
    "from rlkit.infrastructure.dqn_utils import get_env_kwargs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1627358620248,
     "user": {
      "displayName": "Seungyong Moon",
      "photoUrl": "",
      "userId": "08679509530392314245"
     },
     "user_tz": -540
    },
    "id": "vKw5NBlncvMj"
   },
   "outputs": [],
   "source": [
    "#@title 3. Runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'LunarLander-v3' #@param ['LunarLander-v3']\n",
    "  exp_name = 'dqn' #@param\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 32 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
    "  \n",
    "  #@markdown Q-learning parameters\n",
    "  double_q = True #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1337 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq = -1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq = 1000 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args['train_batch_size'] = args['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1627358620248,
     "user": {
      "displayName": "Seungyong Moon",
      "photoUrl": "",
      "userId": "08679509530392314245"
     },
     "user_tz": -540
    },
    "id": "SdijCBlacyal"
   },
   "outputs": [],
   "source": [
    "#@title 4. Create directory for logging\n",
    "\n",
    "base_logdir = \"logs\"\n",
    "exp_name = args[\"exp_name\"] + '_' + args[\"env_name\"]\n",
    "logdir = os.path.join(base_logdir, exp_name)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "args[\"logdir\"] = logdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1627358620249,
     "user": {
      "displayName": "Seungyong Moon",
      "photoUrl": "",
      "userId": "08679509530392314245"
     },
     "user_tz": -540
    },
    "id": "278B1PMWdcZ3"
   },
   "outputs": [],
   "source": [
    "#@title 5. Define Q-function trainer\n",
    "\n",
    "class Q_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n",
    "            'train_batch_size': params['batch_size'],\n",
    "            'double_q': params['double_q'],\n",
    "        }\n",
    "\n",
    "        env_args = get_env_kwargs(params['env_name'])\n",
    "\n",
    "        for k, v in env_args.items():\n",
    "          params[k] = v\n",
    "\n",
    "        self.params['agent_class'] = DQNAgent\n",
    "        self.params['agent_params'] = params\n",
    "        self.params['train_batch_size'] = params['batch_size']\n",
    "        self.params['env_wrappers'] = env_args['env_wrappers']\n",
    "\n",
    "        self.rl_trainer = OffPolicyRLTrainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['num_timesteps'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKdOKM7_desR"
   },
   "outputs": [],
   "source": [
    "#@title 6. Run training\n",
    "\n",
    "#@markdown If your implementation is correct, the average return will be close to or above 50.\n",
    "\n",
    "#@markdown This may take about 30 minutes.\n",
    "\n",
    "trainer = Q_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1627358795337,
     "user": {
      "displayName": "Seungyong Moon",
      "photoUrl": "",
      "userId": "08679509530392314245"
     },
     "user_tz": -540
    },
    "id": "nwxh6MeCdguY"
   },
   "outputs": [],
   "source": [
    "#@title 7. Run Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/$FOLDERNAME/logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPz+nd93LAo8RbeUVx2ofUI",
   "collapsed_sections": [],
   "name": "lunarlander_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
